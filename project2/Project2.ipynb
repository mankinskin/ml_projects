{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Evaluation is key (40 Points)\n",
    "In this project, you we will focus on how to evaluate our machine learning models. Please follow the **TODO**s in this notebook. There are practical and theoretical tasks to do.<br>\n",
    "When working on the tasks please consider the following information:\n",
    "* write short texts in **full sentences** answering the TODOs. Note, that analyzing the theoratical parts of the project give roughly 2/3 of all points\n",
    "* please note that the course language is **English**. German hand-ins are not graded. However, don't worry: we will not substract any points for errors regarding language as long as your report is understandable. \n",
    "* when describing classifiers explain at least the training, testing, and the hyper-parameters\n",
    "* you can use all functions from the sklearn tool box (if not stated otherwise)\n",
    "* **analyzing** means: pointing out differences and similarities and looking for possible reasons\n",
    "* throughout this project use **cross-validation** for splitting up the data for training and testing. More precisely follow the visualizations on slide 12 in slides 2 to split the data. For each split train on the training subsets and test on the test subset. Then average your test results. Following this procedure, your overall results are not dependent on one train-test split and therefore more convincing.\n",
    "\n",
    "You should work in a group of 3. Please enter your names and your TA here.<br>\n",
    "Students:<br>\n",
    "TA:\n",
    "\n",
    "On Monday, **14th December**, there will be a Q&A session in the tutorials! Start to work on this project from now on and take the offer to resolve any remaining ambiguity.\n",
    "This assignment is due on **Sunday, 03.01.2021 11:59pm**. Please upload your solution to the Lernraum+.<br>\n",
    "For a submission you need to be part of a assignment group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics\n",
    "\n",
    "**TODO:** By now you learned more about evaluation. Please define cross-validation and applied it to all following tasks.\n",
    "\n",
    "Cross-validation means at first to split your dataset into k equal sized folds. Then you pick k-1 folds as training\n",
    "set and the remaining as test set. Therefore you have k combinations for training and test set. You can\n",
    "now train and evaluate your model with each combination. By comparing their performances you can retrieve\n",
    "important information. For example if the performance scores are spreading widely(standard deviation), your choosen model is not\n",
    "robust enough.\n",
    "Cross-validation is especially good in situations, where you have little data, but enough computing power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# TODO: load dataset 1\n",
    "data_set = np.load('dataset_1.npz')\n",
    "X = data_set['X']\n",
    "y = data_set['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Describe the Naive Bayes classifier. Why is it called naive?\n",
    "\n",
    "goal: classify for n classes\n",
    "\n",
    "hyperparamters: none\n",
    "training_parameters: pi_c, theta_j_c, my_j_c,  c from {0, number of classes - 1}, j from {0, number of features - 1}\n",
    "testing_parameters: ?\n",
    "\n",
    "The naive Bayes classifier is able to classify n classes. In order to that it learns and optimizes\n",
    "while training with MLE(other options?) the parameter pi_c, which is the the class probability of class c and\n",
    "j * c parameters theta_j_c and my_j_c, which are representing the standard deviation\n",
    "and the mean for each feature and class combination. Therefore every theta_j_c and my_j_c\n",
    "are belonging to own density function in the real-valued case(e. g. Gaussian) and to a probability mass\n",
    "function in a discrete case(e. g. Bernouilli).\n",
    "So one create a relation between one feature and one class, this implies mutually independence of the feature\n",
    "otherwise j must be set of features. The prediction for a new unknown point to be in class c is the product\n",
    "of the probalities of the 'independent' features.\n",
    "\n",
    "It is called naive, because it assumes mutually independence of the features,\n",
    "but the opposite is given in most real world problems, hence there is\n",
    "a correlation between the features (e. g. education and income).\n",
    "In addition to that naive Bayes classifier ignores the Probality\n",
    "of point x (= P(x)) in Bayes' theorem to get a more well to use function.\n",
    "The reason is that P(x) is not depending on the class and the values of the\n",
    "features of x_i are given, so that the denominator is effectively constant\n",
    "(this sentence is Wikipedia quote, more information then the lecture or the lecture notes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN_scores = [0.96 1.   0.96 0.99 1.   0.99 0.98 0.96 0.98 0.98]\n",
      "logReg_scores = [0.43 0.89 0.9  0.9  0.92 0.91 0.9  0.93 0.9  0.37]\n",
      "gaussianNB_scores = [0.95 0.97 0.95 0.96 0.93 0.95 0.96 0.94 0.95 0.94]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# TODO: train a kNN, Logistic Regression, and a Naive Bayes classifier on the dataset. Report the accuracies.\n",
    "\n",
    "kNN = KNeighborsClassifier(5)\n",
    "kNN_scores = cross_val_score(kNN, X, y, cv=10)\n",
    "\n",
    "logReg = LogisticRegression()\n",
    "logReg_scores = cross_val_score(logReg, X, y, cv=10)\n",
    "\n",
    "gaussianNB = GaussianNB()\n",
    "gaussianNB_scores = cross_val_score(gaussianNB, X, y, cv=10)\n",
    "\n",
    "print('kNN_scores = ' + str(kNN_scores))\n",
    "#print(f'kNN_scores = {kNN_scores}')\n",
    "print('logReg_scores = ' + str(logReg_scores))\n",
    "print('gaussianNB_scores = ' + str(gaussianNB_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Describe why the accuracy is not always suitable for evaluating a model's classification performace. Briefly define the alternative metrics from the lecture, and explain in which cases they are more suitable.\n",
    "\n",
    "The accuracy score of two different models can be equal, but depending on the task, there can be different\n",
    "costs for false positives and false negatives. In this cases the accuracy score is not sufficient enough.\n",
    "An example for different costs is autonomous driving:\n",
    "Better stop one time more, then crash into something/someone.\n",
    "Furthermore the accuracy score lacks of a relation to the balance of the number of data points of the classes.\n",
    "Imagine a data set with points with 90% from class 0 and 10% from class 1, in train and test set. Imagine now the\n",
    "naive model, which predicts new data points always as class 0. Then the accuracy score is still 0.9, which at first sounds\n",
    "good, but in fact is not.\n",
    "\n",
    "One alternative is precision, as definite as TP / (TP + FP). This is a good measurement, if you are interested\n",
    "in eliminating FP´s, for example, when FP´s have a higher cost then the FN. When optimizing this, one is interested\n",
    "in reliability of the prediction of class +, without taking into account to how many points are actually in class +.\n",
    "Another alternative is recall with definition TP / (TP + FN). There the missing FP implies, that FP´s have a lower\n",
    "cost then the FN´s. So you are interested in getting as much points of class +, without caring much about FP.\n",
    "The disadvantage of both is, that they are not symmetric, so you get different results, when switching class + and -.\n",
    "If there is a class imbalance, then the class with fewer data points should be class +. Therefore one disadvantage\n",
    "of the accuracy did not vanish fully.\n",
    "The third option is the combination of both: F1 = (Precision * Recall) / (Precision + Recall). Its a good choice, if one\n",
    "wants to optimize Precision and Recall.\n",
    "The fourth options are the true negative rate and the false positive rate, definite by\n",
    "TNR = TN / TN + FP (best value 1, worst 0) and FPR = FP / FP + TN (best value 0, worst 1). They are a good tool, when one has no assumptions about the data set and\n",
    "is at the beginning of a project.\n",
    "The last option for now is the Receiver operating charateristics(=ROC). It is a graph with false positive rate\n",
    "on the x-axis and the true positive rate on the y-axis on [0, 1]^2. For different parameters of models resulting in different\n",
    "combinations of false positive rate and true positive rate and therefore points. The first goal here is to be better then\n",
    "the TPR = FPR line and furthermore to minimize FPR and maximize TPR. You can decide between different models/ROC´s by\n",
    "comparing the area under the curve(AUC). The bigger the AUC the better it is, the best AUC is equal to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: analyze the dataset; focus on possible reasons for why you might want to apply the metrics described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: reevalute your trained models with the metrics you described above.\n",
    "# Visualize the confusion matrices for all classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Describe and analyze your findings. Keep in mind your investigation of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Steps required\n",
    "\n",
    "???\n",
    "Scale your data?\n",
    "Min-max scaling:\n",
    "Standardization: Better for data sets with outlier => calculate standard deviation\n",
    "Robust scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.mean() = [-1.66785369 -1.46791693  0.71401576  2.40657653  0.2238962  -1.19126005\n",
      "  1.78061454]\n",
      "X.std() = [57.42951361 57.58557726 56.65656124  7.39527504 57.76060814  7.45143671\n",
      " 58.64520968]\n",
      "X_scaled_min_max.mean() = [0.49217359 0.49266428 0.50352962 0.46603852 0.50109592 0.49020206\n",
      " 0.50912906]\n",
      "X_scaled_min_max.std() = [0.28764025 0.28811128 0.2833488  0.20001988 0.28892953 0.1780207\n",
      " 0.29338672]\n",
      "X_scaled_standard.mean() = [ 2.79776202e-17 -1.11022302e-17  3.43657394e-17 -7.12763182e-16\n",
      " -4.60187444e-17  2.07611706e-17 -1.68615122e-17]\n",
      "X_scaled_standard.std() = [1. 1. 1. 1. 1. 1. 1.]\n",
      "X_scaled_robust.mean() = [-0.00812176  0.02078122  0.00759399  0.07346281 -0.00337467 -0.05339322\n",
      " -0.01594031]\n",
      "X_scaled_robust.std() = [0.58251288 0.57134835 0.58321217 0.5986119  0.57939802 0.73859548\n",
      " 0.56927511]\n",
      "min-max scaling_score = 0.5198449031736019\n",
      "standard-scaler = 0.5198449031736019\n",
      "robust-scaler = 0.5198449031736019\n",
      "Perfom equally good! Sounds not very likely...!\n",
      "\n",
      "\n",
      "kNN_scores = [0.725 0.84  0.865 0.865 0.85  0.795 0.86  0.785 0.755 0.655]\n",
      "logReg_scores = [0.49  0.815 0.865 0.88  0.89  0.87  0.875 0.87  0.825 0.57 ]\n",
      "gaussianNB_scores = [0.855 0.875 0.855 0.88  0.885 0.875 0.865 0.855 0.86  0.71 ]\n",
      "\n",
      "\n",
      "kNN_scores = [0.82  0.88  0.895 0.91  0.87  0.85  0.9   0.865 0.86  0.745]\n",
      "logReg_scores = [0.505 0.83  0.855 0.875 0.885 0.87  0.87  0.87  0.84  0.515]\n",
      "gaussianNB_scores = [0.855 0.875 0.855 0.88  0.885 0.875 0.865 0.855 0.86  0.71 ]\n",
      "\n",
      "\n",
      "kNN_scores = [0.805 0.89  0.89  0.9   0.865 0.845 0.91  0.88  0.865 0.79 ]\n",
      "logReg_scores = [0.495 0.82  0.865 0.875 0.885 0.87  0.875 0.87  0.84  0.525]\n",
      "gaussianNB_scores = [0.855 0.875 0.855 0.88  0.885 0.875 0.865 0.855 0.86  0.71 ]\n"
     ]
    }
   ],
   "source": [
    "#TODO: Train all three classifiers on the second data set\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "\n",
    "\n",
    "def scoreScaling(x, dim_features):\n",
    "    score = 0\n",
    "    x_stds = np.std(x, axis=0)\n",
    "    for i in range(0, dim_features):\n",
    "        col_i = x[:,i]\n",
    "        distance_interval = max(col_i) - min(col_i)\n",
    "        score = score + (2 * x_stds[i]) / distance_interval\n",
    "\n",
    "    score = score / dim_features\n",
    "    return score\n",
    "\n",
    "data_set = np.load('dataset_2.npz')\n",
    "X = data_set['X']\n",
    "y = data_set['y']\n",
    "\n",
    "\n",
    "print('X.mean() = ' + str(np.mean(X, axis = 0)))\n",
    "print('X.std() = ' + str(np.std(X, axis=0)))\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler.fit(X)\n",
    "X_scaled_min_max = min_max_scaler.transform(X)\n",
    "print('X_scaled_min_max.mean() = ' + str(np.mean(X_scaled_min_max, axis = 0)))\n",
    "print('X_scaled_min_max.std() = ' + str(np.std(X_scaled_min_max, axis=0)))\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "standard_scaler.fit(X)\n",
    "X_scaled_standard = standard_scaler.transform(X)\n",
    "print('X_scaled_standard.mean() = ' + str(np.mean(X_scaled_standard, axis = 0)))\n",
    "print('X_scaled_standard.std() = ' + str(np.std(X_scaled_standard, axis=0)))\n",
    "\n",
    "robust_scaler = RobustScaler()\n",
    "robust_scaler.fit(X)\n",
    "X_scaled_robust = robust_scaler.transform(X)\n",
    "print('X_scaled_robust.mean() = ' + str(np.mean(X_scaled_robust, axis = 0)))\n",
    "print('X_scaled_robust.std() = ' + str(np.std(X_scaled_robust, axis=0)))\n",
    "\n",
    "##choose between scaling by comparing 2 * standart deviation / distance of min-max of values\n",
    "print('min-max scaling_score = ' + str(scoreScaling(X_scaled_min_max, 7)))\n",
    "print('standard-scaler = ' + str(scoreScaling(X_scaled_standard, 7)))\n",
    "print('robust-scaler = ' + str(scoreScaling(X_scaled_robust, 7)))\n",
    "print('Perfom equally good! Sounds not very likely...!')\n",
    "\n",
    "X_list = [X_scaled_min_max, X_scaled_standard, X_scaled_robust]\n",
    "for i in range(0, 3):\n",
    "    kNN = KNeighborsClassifier(5)\n",
    "    kNN_scores = cross_val_score(kNN, X_list[i], y, cv=10)\n",
    "\n",
    "    logReg = LogisticRegression()\n",
    "    logReg_scores = cross_val_score(logReg, X_list[i], y, cv=10)\n",
    "\n",
    "    gaussianNB = GaussianNB()\n",
    "    gaussianNB_scores = cross_val_score(gaussianNB, X_list[i], y, cv=10)\n",
    "\n",
    "    print('\\n')\n",
    "    print('kNN_scores = ' + str(kNN_scores))\n",
    "    print('logReg_scores = ' + str(logReg_scores))\n",
    "    print('gaussianNB_scores = ' + str(gaussianNB_scores))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Analyze the second data set and your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardized data sets\n",
    "\n",
    "A data set $ \\{\\vec{x}_i \\mid i\\in\\{1,\\dots n\\} \\}\\;\\vec{x}_i \\in \\mathbb{R}^{d} $ consisting of $ n $ samples is called *standardized* if each feature $ j $ has zero mean and unit variance. Thus:\n",
    "\n",
    "$$\n",
    "\\mu_j = \\frac{1}{n}\\sum_i (\\vec{x}_i)_j = 0 \\quad \\text{ and }\n",
    "\\sigma^2_j = \\frac{1}{n}\\sum_i \\Big( (\\vec{x}_i)_j - \\mu_j\\Big)^2 = 1 \\quad \\forall\\, j\\in\\{1,\\dots,d\\}.\n",
    "$$\n",
    "\n",
    "**Prove** that the following transformation scales a data set into a standardized one\n",
    "$$(\\vec{x}'_i)_j = \\frac{(\\vec{x}_i)_j - \\mu_j}{\\sigma_j}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show that for any dataset $ X = \\{\\vec{x}_i \\in \\mathbb{R}^{d} \\mid i\\in\\{1,\\dots n\\} \\} $ with $ n, d \\in \\mathbb{N} $, the transformed dataset\n",
    "$$\n",
    "X' = \\{ \\vec{x}'_i \\in \\mathbb{R}^{d} \\mid (\\vec{x}'_i)_j = \\frac{(\\vec{x}_i)_j - \\mu_j}{\\sigma_j}, i \\in\\{1,\\dots n\\}, j \\in\\{1,\\dots d\\} \\}\n",
    "$$\n",
    "is standardized:\n",
    "\n",
    "First, we show that the mean $ \\mu'_j $ of $ X' $ is zero:\n",
    "\\begin{align}\n",
    "\\mu'_j &= \\frac{1}{n}\\sum_i (\\vec{x}'_i)_j = \\frac{1}{n}\\sum_i \\frac{(\\vec{x}_i)_j - \\mu_j}{\\sigma_j} = \\frac{1}{n \\sigma_j}\\sum_i \\Big((\\vec{x}_i)_j - \\mu_j\\Big) = \\frac{1}{n \\sigma_j}\\Big(\\sum_i (\\vec{x}_i)_j - \\sum_i \\mu_j\\Big) \\\\\n",
    "\\sum_i \\mu_j &= \\sum_i \\Big(\\frac{1}{n}\\sum_k (\\vec{x}_k)_j\\Big) = n\\Big(\\frac{1}{n}\\sum_k (\\vec{x}_k)_j\\Big) = \\sum_k (\\vec{x}_k)_j \\\\\n",
    "\\implies \\mu'_j &= \\frac{1}{n \\sigma_j}\\Big(\\sum_i (\\vec{x}_i)_j - \\sum_i (\\vec{x}_i)_j\\Big) = 0\n",
    "\\end{align}\n",
    "Second, we show that the variance $ \\sigma'^2_j $ of $ X' $ is one:\n",
    "\\begin{align}\n",
    "\\sigma'^2_j &= \\frac{1}{n}\\sum_i \\Big( (\\vec{x}'_i)_j - \\mu'_j\\Big)^2 \\\\\n",
    "mu'_j = 0 \\implies \\sigma'^2_j &= \\frac{1}{n}\\sum_i (\\vec{x}'_i)_j^2 = \\frac{1}{n}\\sum_i \\Big(\\frac{(\\vec{x}_i)_j - \\mu_j}{\\sigma_j}\\Big)^2 \\\\ &= \\frac{1}{n\\sigma^2_j}\\sum_i \\Big((\\vec{x}_i)_j - \\mu_j\\Big)^2 \\\\ &= \\frac{1}{\\sigma^2_j}\\frac{1}{n}\\sum_i \\Big((\\vec{x}_i)_j - \\mu_j\\Big)^2 \\\\ &= \\frac{\\sigma^2_j}{\\sigma^2_j} = 1\n",
    "\\end{align}\n",
    "Therefore the dataset $ X' $ is standardized. $\\square$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Try this in practice,\n",
    "#      retrain and test the models on the standardized dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Analyze your results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
