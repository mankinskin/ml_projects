{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfUt7BmNzkIx"
   },
   "source": [
    "# Project 1: First steps in Machine Learning (40 Points)\n",
    "In this project, you will train and test your first machine learning models. Please follow the **TODO**s in this notebook. There are practical and theoretical tasks to do.<br>\n",
    "When working on the tasks please consider the following information:\n",
    "* write short texts in **full sentences** answering the TODOs. Note, that analyzing the theoratical parts of the project give roughly 2/3 of all points\n",
    "* when describing classifiers explain at least the training, testing, and the hyper-parameters\n",
    "* always train your models using the given training sets\n",
    "* evaluate the models using the given test sets\n",
    "* have a look at all imports in this notebook; they already define which method you should use\n",
    "\n",
    "You should work in a group of 3. Please enter your names and your TA here.<br>\n",
    "Students: Basel Ammo & Linus Behrbohm<br>\n",
    "TA: Yasir Plückebaum\n",
    "\n",
    "On Monday, **7th December**, there will be a Q&A session in the tutorials! Start to work on this project from now on and take the offer to resolve any remaining ambiguity.\n",
    "This assignment is due on **Sunday, 29.11.2020 11:59pm**. Please upload your solution to the Lernraum+.<br>\n",
    " For a submission you need to be part of a assignment group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCypy1U8zkI0"
   },
   "source": [
    "## kNN - Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "197KcJbGzkI3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# load dataset 1\n",
    "data_set1 = np.load('dataset_1.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dataset is split into a training and a test dataset with input and expected output data each. The format of the input data is a 2D vector, and the output format is an integer with a value of 0 or 1. There are 536 samples in the training data set and 264 samples in the test data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "0,5/1P , Anzahl Datenpunkte pro Klasse vergessen\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pp0Fida7zkJA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: take a closer look at the dataset, e.g. number of samples, dimensionality, labels, etc.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# generic function to extract named properties from the data set items\n",
    "# fs is a list of tuples, first a name, then a function mapping an item to a property\n",
    "def property_table(fs, data_set):\n",
    "    keys = list(data_set.keys()) # row names\n",
    "    props = list(map(lambda f: f[0], fs)) # column names\n",
    "    funcs = list(map(lambda f: f[1], fs)) # property functions\n",
    "    items = list(map(lambda t: t[1], data_set.items())) # data items\n",
    "    data = zip(*list(map(lambda f: map(f, items), funcs))) # each prop for each item\n",
    "    return pd.DataFrame(data, index=keys, columns=props) # use pandas.DataFrame to print a table\n",
    "    \n",
    "data_frame = property_table([\n",
    "        ('Sample Count', lambda item: len(item)),\n",
    "        ('Shape', lambda item: item.shape),\n",
    "        ('Type', lambda item: type(item[0])),\n",
    "        ('First', lambda item: item[0]),\n",
    "    ], data_set1)\n",
    "\n",
    "print(data_frame)\n",
    "def plot_data(data_set):\n",
    "    X_train = data_set['X_train']\n",
    "    X_test = data_set['X_test']\n",
    "    y_train = data_set['y_train']\n",
    "    y_test = data_set['y_test']\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    fig.set_figwidth(15)\n",
    "    axs[0].scatter(*zip(*X_train), c=y_train)\n",
    "    axs[0].set_title(\"Training\")\n",
    "\n",
    "    axs[1].scatter(*zip(*X_test), c=y_test)\n",
    "    axs[1].set_title(\"Test\")\n",
    "\n",
    "plot_data(data_set1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "Sehr sauber! 1(+0,5)P. Muss zugeben, ich dachte kurz der Code wäre aus dem Internet\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-G-3Qj8zkJH"
   },
   "source": [
    "### the Model\n",
    "\n",
    "The kNN-Classifier learns a probability distribution in a space based on a set of input points with previously assigned classes, i.e. by supervised training. There can be any number of classes. The kNN-Classifier calculates the probability of any new point belonging to a specific class as the proportion of points from the k nearest known points with that specific class. I.e. for any given point x, the probability to belong to a specific class c is calculated by the number of points with class c in the nearest k points of x, divided by k. The predicted class by the kNN-Classifier is the class with the maximum probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "    1/3P , K als Hyperparameter und Training vergessen. (In wie weit lernt Knn eine Wsk- Verteilung?)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yoR7mM5o81n-"
   },
   "outputs": [],
   "source": [
    "# TODO: train a kNN classifier with k=5 on the training set and test it with the test set\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import functools\n",
    "foldl = lambda acc, f, xs: functools.reduce(f, xs, acc)\n",
    "\n",
    "def train_classifier(clf, data_set):\n",
    "    X_train = data_set['X_train']\n",
    "    y_train = data_set['y_train']\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf\n",
    "\n",
    "def test_classifier(clf, data_set):\n",
    "    X_test = data_set['X_test']\n",
    "    y_test = data_set['y_test']\n",
    "    y=clf.predict(X_test)\n",
    "    # ensure y_test.size != 0\n",
    "    if y_test.size == 0: return 1\n",
    "    # count number of correct predictions and divide them by total number of samples\n",
    "    return foldl(0, lambda acc, t: acc + (t[0]==t[1]), zip(y, y_test))/y_test.size\n",
    "\n",
    "def train_and_test_classifier(clf, data_set):\n",
    "    clf = train_classifier(clf, data_set)\n",
    "    return test_classifier(clf, data_set)\n",
    "\n",
    "def test_data_set(clf, data_set):\n",
    "    accuracy = train_and_test_classifier(clf, data_set)\n",
    "    print(f'Accuracy {accuracy}')\n",
    "    \n",
    "test_data_set(KNeighborsClassifier(n_neighbors=5), data_set1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "    1P\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kNN-Classifier with k=5 reached a 98% accuracy on the test data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "1P\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FY4-FS3LzkJI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: train and test the kNN classifier for different values of k on dataset 1.\n",
    "#       Plot the accuracy for different values of k. Choose usefull lower and upper bounds for k.\n",
    "import matplotlib.pyplot as plt\n",
    "lo = 1\n",
    "hi = 536\n",
    "accuracies = list(map(lambda k: train_and_test_classifier(KNeighborsClassifier(n_neighbors=k), data_set1), range(lo,hi)))\n",
    "plt.plot(accuracies)\n",
    "max_index = max(range(0, len(accuracies)), key=lambda i: accuracies[i])\n",
    "print('The best choice for k in [{}, {}] is {} with an accuracy of {}'.format(lo, hi, max_index+lo, accuracies[max_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>1P</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoj8y_WWzkJQ"
   },
   "source": [
    "**TODO:** Answer the following questions in a **full text**.\n",
    "\n",
    "* Describe your choice of values of k. Why did you choose them?\n",
    "* For which values of k does the model perform best?\n",
    "* Would this value perform best on another dataset as well?\n",
    "* How can k be choosen?\n",
    "* What is the smallest and the greatest possible value for k? What would happen if we would choose these special values?\n",
    "\n",
    "I chose values 1 between 536, because 1 is the minimum number of neighbors possible and 536 is the number of samples in the data set, so that would be the maximum number of neighbors possible in this data set. The model turned out to perform best for k=5. This may be different for other data sets, and it depends on the number of samples in the data set, aswell as the distribution of the classes being learned. This has to be learned for each data set, by trying out different values for k. A rule of thumb might be, if you have a lot of data already, you can choose lower values of k, because the boundaries between the classes will already be well distiguished in the data you already have. Higher values of k are more sensitive to unbalanced data sets, whereas smaller values for k are more sensitive to noisy data sets. As mentioned, the smallest possible value for k is 1, as this is the minimum number of neighbors possible. The largest possible value is the number of points in the data set, as this is the maximum number of neighbors a point can have in any data set. Note that the point being predicted is not part of the data set we already know the classes of. If we chose k=1 we would classify each new point as the same class as the closest known point next to it. If we chose k as the total number of samples in data set, we would predict every new point to be of the class that is most frequent in the data set, since all points are considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color='red'>\n",
    "    1,5/2P\n",
    "    -0,5P \"How can k be choosen?\" Das wollte auf cross-validation und Bayesian techniques hinaus</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>7,5/10P</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxex3z5tzkJR"
   },
   "source": [
    "## Logistic Regression\n",
    "Let's try another model as well.\n",
    "\n",
    "### the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhB-OKu6zkJS"
   },
   "outputs": [],
   "source": [
    "# TODO: load dataset_2.npz and analyze the dataset\n",
    "data_set2 = np.load('dataset_2.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yr-DXalVA60W"
   },
   "source": [
    "The second dataset is also split into a training and a test data set with input and expected output data each. The format of the input data is a 2D vector, and the output format is an integer with a value of 0 or 1. There are 670 samples in the training data set and 330 samples in the test data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>0,5/1P, samples per class fehlt</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = property_table([\n",
    "        ('Sample Count', lambda item: len(item)),\n",
    "        ('Shape', lambda item: item.shape),\n",
    "        ('Type', lambda item: type(item[0])),\n",
    "        ('First', lambda item: item[0]),\n",
    "    ], data_set2)\n",
    "\n",
    "print(data_frame)\n",
    "\n",
    "plot_data(data_set2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>1P</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dygg6SnkzkJX"
   },
   "source": [
    "### the Model\n",
    "\n",
    "The logistic regression model finds a linear decision boundary by applying gradient descent to a loss function of the likelihood of a point belonging to a specific class: `L(P(y=1|x))` This loss function is usually the negative logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>1/3P, es fehlt training und predicting</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nBBLXJMBSqI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: train and test logistic regression on dataset_2.npz (using the training and test set),\n",
    "# plot the dataset and the decision boundary, own implementation needed (see lecture slide 51 in slides1.pdf)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = train_classifier(LogisticRegression(), data_set2)\n",
    "accuracy = test_classifier(clf, data_set2)\n",
    "print(accuracy)\n",
    "def plot_decision_boundary(clf, data_set):\n",
    "\n",
    "    X_train = data_set['X_train']\n",
    "    X_test = data_set['X_test']\n",
    "    y_train = data_set['y_train']\n",
    "    y_test = data_set['y_test']\n",
    "    X = np.concatenate((X_train, X_test))\n",
    "    y = np.concatenate((y_train, y_test))\n",
    "    xmin, ymin = np.min(X, axis=0)\n",
    "    xmax, ymax = np.max(X, axis=0)\n",
    "    b = clf.intercept_[0] # bias\n",
    "    w1, w2 = clf.coef_.T # weights in W\n",
    "    \n",
    "    c = -b/w2 # x axis intercept of decision boundary\n",
    "    m = -w1/w2 # slope of decision boundary\n",
    "    print(f'{m} {c}')\n",
    "    xd = np.array([xmin, xmax])\n",
    "    yd = m*xd + c # decision boundary\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 1)\n",
    "    fig.set_figwidth(15)\n",
    "    axs.scatter(*zip(*X), c=y)\n",
    "    axs.plot(xd, yd, lw=1, ls='--')\n",
    "    axs.set_title(\"Decision Boundary\")\n",
    "\n",
    "plot_decision_boundary(clf, data_set2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>2P</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model converged to a decision boundary with `y = 0.518*x - 0.136`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>0,5/1P</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check whether w separates the two classes.\n",
    "#       How does the negative log-likelihood (NLL) change for αw as α goes to infinity?\n",
    "#\n",
    "# Hint: Implement an own function which computes the NLL for a new w, and report the NLL for different values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** What can you infer from your observations regarding the training of a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### thoeretical considerations\n",
    "\n",
    "**TODO:** Prove that \n",
    "\n",
    "$$\\lim_{\\alpha\\to\\infty}-\\log\\left(P\\left(\\left.\\mathcal{D}\\right|\\vec{w}\\right)\\right)=0$$\n",
    "if $\\vec{w}$ splits the data perfectly where $\\mathcal{D}$ represents the data.\n",
    "\n",
    "**Hint:** You can analyze both cases, y=0 and y=1, and find a limit of the likelihood for one sample. Then, calculate the limit of the negative log-likelihood.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>0/12P</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>5/20P</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edHXCg-IzkJg"
   },
   "source": [
    "## Comparing kNN and Logistic Regression\n",
    "Finally, we want you to compare the kNN and the logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOIj3_vUzkJh"
   },
   "outputs": [],
   "source": [
    "# TODO: train both kNN and logistic regression on the training set for both datasets.\n",
    "#       Evaluate the models using the respective test set.\n",
    "# Plot the data and the decision boundary of the classifiers. plot_2d_decisionboundary() in utils.py can be used.\n",
    "\n",
    "from utils import plot_2d_decisionboundary\n",
    "from utils import plot_classification_dataset\n",
    "\n",
    "print('### kNN ###')\n",
    "clf = KNeighborsClassifier(n_neighbors=5)\n",
    "print('Dataset 1:')\n",
    "test_data_set(clf, data_set1)\n",
    "print('Dataset 2:')\n",
    "test_data_set(clf, data_set2)\n",
    "print('\\n')\n",
    "\n",
    "print('### Logistic Regression ###')\n",
    "clf = LogisticRegression()\n",
    "print('Dataset 1:')\n",
    "test_data_set(clf, data_set1)\n",
    "plot_decision_boundary(clf, data_set1)\n",
    "print('Dataset 2:')\n",
    "test_data_set(clf, data_set2)\n",
    "plot_decision_boundary(clf, data_set2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>3P</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANBVeSXjzkJn"
   },
   "source": [
    "The kNN Classifier performs well on both of these data set. The second dataset is easily separable, which also makes it easy for the kNN to find the boundary between the classes. The logistic regression model performs well on the linearly separable data set (data_set2) but performs worse than the kNN on the first data set, because it is not linearly seperable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>6/7P, \"The second dataset is easily separable, which also makes it easy for the kNN to find the boundary between the classes.\" Warum?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>9/10P</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>21.5/40P</font>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "HCypy1U8zkI0",
    "qxex3z5tzkJR",
    "edHXCg-IzkJg"
   ],
   "name": "Copy of Project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
